{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b9004e",
   "metadata": {
    "id": "e0b9004e"
   },
   "source": [
    "# Signature detection with custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ssy33ooGcc_c",
   "metadata": {
    "id": "ssy33ooGcc_c"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3690112f",
   "metadata": {
    "executionInfo": {
     "elapsed": 15187,
     "status": "ok",
     "timestamp": 1762186027804,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "3690112f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.retinanet import RetinaNet_ResNet50_FPN_Weights\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms\n",
    "import webdataset as wds\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b29eb17a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1762186027833,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "b29eb17a",
    "outputId": "72d11683-1c94-4604-b08b-ccd1250aa8e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: NVIDIA GeForce MX450\n"
     ]
    }
   ],
   "source": [
    "# check GPU\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b00ba",
   "metadata": {
    "id": "489b00ba"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb3249ea",
   "metadata": {
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1762186157190,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "bb3249ea"
   },
   "outputs": [],
   "source": [
    "train_dataset = \"datasets/custom_augmented/train-00000.tar\"\n",
    "val_dataset = \"datasets/custom_augmented/val-00000.tar\"\n",
    "test_dataset = \"datasets/custom_augmented/test-00000.tar\"\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 2  # 1 class (signature) + background\n",
    "imgsz = 256\n",
    "epochs = 2\n",
    "batch_size = 4\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51a3b37",
   "metadata": {
    "id": "c51a3b37"
   },
   "source": [
    "### utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0828b341",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1762186059603,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "0828b341"
   },
   "outputs": [],
   "source": [
    "def iou(boxA, boxB):\n",
    "    # boxes are [x1,y1,x2,y2]\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "    boxAArea = max(0, (boxA[2] - boxA[0])) * max(0, (boxA[3] - boxA[1]))\n",
    "    boxBArea = max(0, (boxB[2] - boxB[0])) * max(0, (boxB[3] - boxB[1]))\n",
    "    denom = float(boxAArea + boxBArea - interArea)\n",
    "    return interArea / denom if denom > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9732329f",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1762186059605,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "9732329f"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    it = 0\n",
    "    for images, targets in loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += losses.item()\n",
    "        it += 1\n",
    "    return running_loss / max(1, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bb11d2c",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1762186059619,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "2bb11d2c"
   },
   "outputs": [],
   "source": [
    "def validate(model, loader, device):\n",
    "    model.train()\n",
    "    val_loss = 0.0\n",
    "    it = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Temporarily run in train mode to get loss dict (model() in eval returns list)\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += losses.item()\n",
    "            it += 1\n",
    "\n",
    "    return val_loss / max(1, it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "755a1956",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1762186059651,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "755a1956"
   },
   "outputs": [],
   "source": [
    "def evaluate_precision_recall(model, loader, device, iou_th=0.5, score_th=0.5):\n",
    "    model.eval()\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            img = images[0].to(device)\n",
    "            gt = targets[0]\n",
    "            preds = model([img])[0]\n",
    "            pred_boxes = preds['boxes'].cpu().numpy()\n",
    "            pred_scores = preds['scores'].cpu().numpy()\n",
    "            gt_boxes = gt['boxes'].cpu().numpy() if gt['boxes'].size(0) > 0 else np.zeros((0,4))\n",
    "\n",
    "            keep_idx = np.where(pred_scores >= score_th)[0]\n",
    "            pred_boxes = pred_boxes[keep_idx]\n",
    "            matched_gt = set()\n",
    "            for pb in pred_boxes:\n",
    "                best_iou = 0\n",
    "                best_j = -1\n",
    "                for j, gb in enumerate(gt_boxes):\n",
    "                    if j in matched_gt:\n",
    "                        continue\n",
    "                    cur_iou = iou(pb, gb)\n",
    "                    if cur_iou > best_iou:\n",
    "                        best_iou = cur_iou\n",
    "                        best_j = j\n",
    "                if best_iou >= iou_th and best_j >= 0:\n",
    "                    TP += 1\n",
    "                    matched_gt.add(best_j)\n",
    "                else:\n",
    "                    FP += 1\n",
    "            FN += (len(gt_boxes) - len(matched_gt))\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    return precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ccf4468",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1762186059662,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "2ccf4468"
   },
   "outputs": [],
   "source": [
    "def training_loop(model, train_loader, val_loader, optimizer, lr_scheduler, device, epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "            lr_scheduler.step()\n",
    "            val_loss = validate(model, val_loader, device)\n",
    "            prec, rec = evaluate_precision_recall(model, val_loader, device)\n",
    "            print(f'Epoch {epoch+1}/{epochs} — train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}, prec: {prec:.3f}, rec: {rec:.3f}, time: {time.time()-t0:.1f}s')\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e).lower():\n",
    "                print('RuntimeError: CUDA out of memory during training.\\nConsider:')\n",
    "                # try to free cache and continue or abort\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                raise\n",
    "            else:\n",
    "                raise\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c9613",
   "metadata": {
    "id": "924c9613"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f081a31",
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1762186158777,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "7f081a31"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((imgsz, imgsz)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def preprocessSample(sample):\n",
    "     # Detect image key dynamically\n",
    "    img_key = None\n",
    "    for k in sample.keys():\n",
    "        if k.lower() in [\"jpg\", \"jpeg\", \"png\"]:\n",
    "            img_key = k\n",
    "            break\n",
    "    if img_key is None:\n",
    "        raise ValueError(f\"No supported image format found in sample keys: {list(sample.keys())}\")\n",
    "\n",
    "    # Image already decoded to PIL\n",
    "    img = sample[img_key]\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "    # Get original image size before resizing\n",
    "    orig_w, orig_h = img.size\n",
    "\n",
    "    # Resize image\n",
    "    img_resized = transforms.Resize((imgsz, imgsz))(img)\n",
    "    new_w, new_h = img_resized.size\n",
    "\n",
    "    # Compute scale factors\n",
    "    scale_x = new_w / orig_w\n",
    "    scale_y = new_h / orig_h\n",
    "\n",
    "    # Parse target\n",
    "    target = sample[\"json\"]\n",
    "\n",
    "    # Convert boxes [x, y, w, h] → [x1, y1, x2, y2]\n",
    "    boxes = []\n",
    "    for (x, y, w, h) in target[\"boxes\"]:\n",
    "        x1, y1, x2, y2 = x, y, x + w, y + h\n",
    "        # Scale coordinates\n",
    "        x1 *= scale_x\n",
    "        x2 *= scale_x\n",
    "        y1 *= scale_y\n",
    "        y2 *= scale_y\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "    # Convert to tensors\n",
    "    target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    target[\"labels\"] = torch.as_tensor(target[\"labels\"], dtype=torch.int64)\n",
    "\n",
    "    # Final transform to tensor\n",
    "    img_tensor = transforms.ToTensor()(img_resized)\n",
    "\n",
    "    return img_tensor, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "930633f7",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1762186158780,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "930633f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matti\\anaconda3\\envs\\ai\\Lib\\site-packages\\webdataset\\compat.py:379: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
      "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = (\n",
    "    wds.WebDataset(train_dataset)   # <- use pattern or list of tar paths\n",
    "    .decode(\"pil\")\n",
    "    .map(preprocessSample)\n",
    ")\n",
    "val_dataset = (\n",
    "    wds.WebDataset(val_dataset)   # <- use pattern or list of tar paths\n",
    "    .decode(\"pil\")\n",
    "    .map(preprocessSample)\n",
    ")\n",
    "test_dataset = (\n",
    "    wds.WebDataset(test_dataset)   # <- use pattern or list of tar paths\n",
    "    .decode(\"pil\")\n",
    "    .map(preprocessSample)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb27f7f",
   "metadata": {
    "id": "5bb27f7f"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "401a18a9",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1762186389738,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "401a18a9"
   },
   "outputs": [],
   "source": [
    "class SimpleBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_anchors, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_anchors = num_anchors\n",
    "        self.num_classes = num_classes\n",
    "        self.loc = nn.Conv2d(in_channels, num_anchors * 4, 1)\n",
    "        self.cls = nn.Conv2d(in_channels, num_anchors * num_classes, 1)\n",
    "    def forward(self, x):\n",
    "        locs = self.loc(x)\n",
    "        clss = self.cls(x)\n",
    "        # reshape to (batch, anchors, 4) and (batch, anchors, num_classes)\n",
    "        batch = x.shape[0]\n",
    "        locs = locs.permute(0,2,3,1).contiguous().view(batch, -1, 4)\n",
    "        clss = clss.permute(0,2,3,1).contiguous().view(batch, -1, self.num_classes)\n",
    "        return locs, clss\n",
    "\n",
    "class CustomDetector(nn.Module):\n",
    "    def __init__(self, num_classes, anchor_stride=32):\n",
    "        super().__init__()\n",
    "        self.backbone = SimpleBackbone()\n",
    "        # Calculate feature map size\n",
    "        self.anchor_stride = anchor_stride\n",
    "        fmap_size = imgsz // anchor_stride\n",
    "        self.num_anchors = fmap_size * fmap_size\n",
    "        self.head = DetectionHead(128, self.num_anchors, num_classes)\n",
    "        # Generate anchor centers\n",
    "        grid = torch.meshgrid(\n",
    "            torch.arange(fmap_size), torch.arange(fmap_size), indexing='ij'\n",
    "        )\n",
    "        self.register_buffer('anchor_centers', torch.stack(grid, dim=-1).reshape(-1,2).float() * anchor_stride)\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        locs, clss = self.head(feats)\n",
    "        # anchors are at self.anchor_centers\n",
    "        return locs, clss, self.anchor_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bcb87",
   "metadata": {
    "id": "ba8bcb87"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77ab98f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1762186393113,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "77ab98f8",
    "outputId": "f8db81d5-0668-4936-bb4c-6ea2e22050ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomDetector(\n",
       "  (backbone): SimpleBackbone(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (7): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (head): DetectionHead(\n",
       "    (loc): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (cls): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomDetector(num_classes=2)\n",
    "\n",
    "try:\n",
    "    model.to(device)\n",
    "except RuntimeError as e:\n",
    "    print('Error moving model to device — falling back to CPU.\\n', e)\n",
    "    device = torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ec2be2e",
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1762186394132,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "0ec2be2e"
   },
   "outputs": [],
   "source": [
    "# clear cache and reduce fragmentation\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61d0aac7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "executionInfo": {
     "elapsed": 548,
     "status": "error",
     "timestamp": 1762186395566,
     "user": {
      "displayName": "Mattia Lodi",
      "userId": "04236055631688529155"
     },
     "user_tz": -60
    },
    "id": "61d0aac7",
    "outputId": "2357b3e2-b7f0-44da-fae6-af0fe7004b30"
   },
   "outputs": [],
   "source": [
    "# Custom training loop for CustomDetector\n",
    "def custom_loss(locs, clss, targets, anchors, device):\n",
    "    # targets: list of dicts with 'boxes' and 'labels'\n",
    "    batch_size = len(targets)\n",
    "    loc_loss = 0.0\n",
    "    cls_loss = 0.0\n",
    "    for i in range(batch_size):\n",
    "        gt_boxes = targets[i]['boxes'].to(device)\n",
    "        gt_labels = targets[i]['labels'].to(device)\n",
    "        # For simplicity, match each GT to closest anchor\n",
    "        if gt_boxes.numel() == 0:\n",
    "            continue\n",
    "        anchor_centers = anchors.to(device)\n",
    "        pred_locs = locs[i]\n",
    "        pred_clss = clss[i]\n",
    "        for j, gt_box in enumerate(gt_boxes):\n",
    "            # Find closest anchor\n",
    "            anchor_dists = torch.norm(anchor_centers - gt_box[:2], dim=1)\n",
    "            anchor_idx = torch.argmin(anchor_dists)\n",
    "            # Box regression loss (MSE)\n",
    "            loc_loss += F.mse_loss(pred_locs[anchor_idx], gt_box, reduction='mean')\n",
    "            # Classification loss (CrossEntropy)\n",
    "            cls_loss += F.cross_entropy(pred_clss[anchor_idx].unsqueeze(0), gt_labels[j].unsqueeze(0))\n",
    "    return loc_loss + cls_loss\n",
    "\n",
    "def train_custom_detector(model, loader, optimizer, device, epochs=2):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        for images, targets in loader:\n",
    "            images = torch.stack([img.to(device) for img in images])\n",
    "            locs, clss, anchors = model(images)\n",
    "            loss = custom_loss(locs, clss, targets, anchors, device)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        avg_loss = running_loss / max(1, batch_count)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} — loss: {avg_loss:.4f}\")\n",
    "        losses.append(avg_loss)\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72a71365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/2 — loss: nan\n",
      "Epoch 1/2 — loss: nan\n",
      "Epoch 2/2 — loss: nan\n",
      "Epoch 2/2 — loss: nan\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "model, train_losses = train_custom_detector(model, train_loader, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772483e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(img_tensor, boxes, color=(0,255,0), linewidth=2):\n",
    "    img = (img_tensor.permute(1,2,0).cpu().numpy() * 255).astype(np.uint8).copy()\n",
    "    for b in boxes:\n",
    "        x1, y1, x2, y2 = map(int, b)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, linewidth)\n",
    "    return img\n",
    "\n",
    "n=5\n",
    "score_thresh=0.5\n",
    "model.eval()\n",
    "plt.figure(figsize=(12, 4 * n))\n",
    "\n",
    "cnt = 1\n",
    "with torch.no_grad():\n",
    "    iter_dataset = iter(test_dataset)\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            img_tensor, target = next(iter_dataset)\n",
    "        except StopIteration:\n",
    "            print(\"End of dataset reached.\")\n",
    "            break\n",
    "\n",
    "        img = img_tensor.to(device)\n",
    "        preds = model([img])[0]\n",
    "\n",
    "        pred_boxes = preds['boxes'].cpu().numpy()\n",
    "        pred_scores = preds['scores'].cpu().numpy()\n",
    "        keep = pred_scores >= score_thresh\n",
    "        pred_boxes = pred_boxes[keep]\n",
    "\n",
    "        gt_boxes = (\n",
    "            target['boxes'].numpy()\n",
    "            if target[\"boxes\"].numel() > 0\n",
    "            else np.zeros((0, 4))\n",
    "        )\n",
    "\n",
    "        # Draw boxes: GT (green), Predictions (red)\n",
    "        vis_gt = draw_boxes(img_tensor, gt_boxes, color=(0,255,0))\n",
    "        vis_pred = draw_boxes(img_tensor, pred_boxes, color=(0,0,255))\n",
    "\n",
    "        # Combine side-by-side\n",
    "        combined = np.concatenate([vis_gt, vis_pred], axis=1)\n",
    "        plt.subplot(n, 1, cnt)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"GT (green) | Pred (red) — Sample {i}\")\n",
    "        plt.imshow(combined[:,:,::-1])\n",
    "        cnt += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ssy33ooGcc_c",
    "c51a3b37",
    "924c9613"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
