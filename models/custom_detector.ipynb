{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b9004e",
   "metadata": {},
   "source": [
    "# Signature detection with custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3690112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.retinanet import RetinaNet_ResNet50_FPN_Weights\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms\n",
    "import webdataset as wds\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29eb17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check GPU\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b00ba",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3249ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"datasets/custom/train-00000.tar\"\n",
    "val_dataset = \"datasets/custom/val-00000.tar\"\n",
    "test_dataset = \"datasets/custom/test-00000.tar\"\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 2  # 1 class (signature) + background\n",
    "imgsz = 512\n",
    "epochs = 2\n",
    "batch_size = 4\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51a3b37",
   "metadata": {},
   "source": [
    "### utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(boxA, boxB):\n",
    "    # boxes are [x1,y1,x2,y2]\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "    boxAArea = max(0, (boxA[2] - boxA[0])) * max(0, (boxA[3] - boxA[1]))\n",
    "    boxBArea = max(0, (boxB[2] - boxB[0])) * max(0, (boxB[3] - boxB[1]))\n",
    "    denom = float(boxAArea + boxBArea - interArea)\n",
    "    return interArea / denom if denom > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    it = 0\n",
    "    for images, targets in loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += losses.item()\n",
    "        it += 1\n",
    "    return running_loss / max(1, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb11d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, device):\n",
    "    model.train()\n",
    "    val_loss = 0.0\n",
    "    it = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Temporarily run in train mode to get loss dict (model() in eval returns list)\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += losses.item()\n",
    "            it += 1\n",
    "\n",
    "    return val_loss / max(1, it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_precision_recall(model, loader, device, iou_th=0.5, score_th=0.5):\n",
    "    model.eval()\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            img = images[0].to(device)\n",
    "            gt = targets[0]\n",
    "            preds = model([img])[0]\n",
    "            pred_boxes = preds['boxes'].cpu().numpy()\n",
    "            pred_scores = preds['scores'].cpu().numpy()\n",
    "            gt_boxes = gt['boxes'].cpu().numpy() if gt['boxes'].size(0) > 0 else np.zeros((0,4))\n",
    "\n",
    "            keep_idx = np.where(pred_scores >= score_th)[0]\n",
    "            pred_boxes = pred_boxes[keep_idx]\n",
    "            matched_gt = set()\n",
    "            for pb in pred_boxes:\n",
    "                best_iou = 0\n",
    "                best_j = -1\n",
    "                for j, gb in enumerate(gt_boxes):\n",
    "                    if j in matched_gt:\n",
    "                        continue\n",
    "                    cur_iou = iou(pb, gb)\n",
    "                    if cur_iou > best_iou:\n",
    "                        best_iou = cur_iou\n",
    "                        best_j = j\n",
    "                if best_iou >= iou_th and best_j >= 0:\n",
    "                    TP += 1\n",
    "                    matched_gt.add(best_j)\n",
    "                else:\n",
    "                    FP += 1\n",
    "            FN += (len(gt_boxes) - len(matched_gt))\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    return precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_loader, val_loader, optimizer, lr_scheduler, device, epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "            lr_scheduler.step()\n",
    "            val_loss = validate(model, val_loader, device)\n",
    "            prec, rec = evaluate_precision_recall(model, val_loader, device)\n",
    "            print(f'Epoch {epoch+1}/{epochs} — train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}, prec: {prec:.3f}, rec: {rec:.3f}, time: {time.time()-t0:.1f}s')\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e).lower():\n",
    "                print('RuntimeError: CUDA out of memory during training.\\nConsider:')\n",
    "                # try to free cache and continue or abort\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                raise\n",
    "            else:\n",
    "                raise\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c9613",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f081a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((imgsz, imgsz)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def preprocessSample(sample):\n",
    "     # Detect image key dynamically\n",
    "    img_key = None\n",
    "    for k in sample.keys():\n",
    "        if k.lower() in [\"jpg\", \"jpeg\", \"png\"]:\n",
    "            img_key = k\n",
    "            break\n",
    "    if img_key is None:\n",
    "        raise ValueError(f\"No supported image format found in sample keys: {list(sample.keys())}\")\n",
    "\n",
    "    # Image already decoded to PIL\n",
    "    img = sample[img_key]\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "    # Get original image size before resizing\n",
    "    orig_w, orig_h = img.size\n",
    "\n",
    "    # Resize image\n",
    "    img_resized = transforms.Resize((imgsz, imgsz))(img)\n",
    "    new_w, new_h = img_resized.size\n",
    "\n",
    "    # Compute scale factors\n",
    "    scale_x = new_w / orig_w\n",
    "    scale_y = new_h / orig_h\n",
    "\n",
    "    # Parse target\n",
    "    target = sample[\"json\"]\n",
    "\n",
    "    # Convert boxes [x, y, w, h] → [x1, y1, x2, y2]\n",
    "    boxes = []\n",
    "    for (x, y, w, h) in target[\"boxes\"]:\n",
    "        x1, y1, x2, y2 = x, y, x + w, y + h\n",
    "        # Scale coordinates\n",
    "        x1 *= scale_x\n",
    "        x2 *= scale_x\n",
    "        y1 *= scale_y\n",
    "        y2 *= scale_y\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "    # Convert to tensors\n",
    "    target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    target[\"labels\"] = torch.as_tensor(target[\"labels\"], dtype=torch.int64)\n",
    "\n",
    "    # Final transform to tensor\n",
    "    img_tensor = transforms.ToTensor()(img_resized)\n",
    "\n",
    "    return img_tensor, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930633f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matti\\anaconda3\\envs\\ai\\Lib\\site-packages\\webdataset\\compat.py:379: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
      "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = (\n",
    "    wds.WebDataset(train_dataset)   # <- use pattern or list of tar paths\n",
    "    .decode(\"pil\")\n",
    "    .map(preprocessSample)\n",
    ")\n",
    "val_dataset = (\n",
    "    wds.WebDataset(val_dataset)   # <- use pattern or list of tar paths\n",
    "    .decode(\"pil\")\n",
    "    .map(preprocessSample)\n",
    ")\n",
    "test_dataset = (\n",
    "    wds.WebDataset(test_dataset)   # <- use pattern or list of tar paths\n",
    "    .decode(\"pil\")\n",
    "    .map(preprocessSample)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=0, \n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=0, \n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb27f7f",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # CNNs for rgb images\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5)\n",
    "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=48, kernel_size=5)\n",
    "        self.conv5 = nn.Conv2d(in_channels=48, out_channels=192, kernel_size=5)\n",
    "\n",
    "        # Connecting CNN outputs with Fully Connected layers for classification\n",
    "        self.class_fc1 = nn.Linear(in_features=1728, out_features=240)\n",
    "        self.class_fc2 = nn.Linear(in_features=240, out_features=120)\n",
    "        self.class_out = nn.Linear(in_features=120, out_features=2)\n",
    "\n",
    "        # Connecting CNN outputs with Fully Connected layers for bounding box\n",
    "        self.box_fc1 = nn.Linear(in_features=1728, out_features=240)\n",
    "        self.box_fc2 = nn.Linear(in_features=240, out_features=120)\n",
    "        self.box_out = nn.Linear(in_features=120, out_features=4)\n",
    "\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv3(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv4(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv5(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.avg_pool2d(t, kernel_size=4, stride=2)\n",
    "\n",
    "        t = torch.flatten(t,start_dim=1)\n",
    "        \n",
    "\n",
    "        class_t = self.class_fc1(t)\n",
    "        class_t = F.relu(class_t)\n",
    "\n",
    "        class_t = self.class_fc2(class_t)\n",
    "        class_t = F.relu(class_t)\n",
    "\n",
    "        class_t = F.softmax(self.class_out(class_t),dim=1)\n",
    "\n",
    "        box_t = self.box_fc1(t)\n",
    "        box_t = F.relu(box_t)\n",
    "\n",
    "        box_t = self.box_fc2(box_t)\n",
    "        box_t = F.relu(box_t)\n",
    "\n",
    "        box_t = self.box_out(box_t)\n",
    "        box_t = F.sigmoid(box_t)\n",
    "\n",
    "        return [class_t,box_t]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bcb87",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "\n",
    "try:\n",
    "    model.to(device)\n",
    "except RuntimeError as e:\n",
    "    print('Error moving model to device — falling back to CPU.\\n', e)\n",
    "    device = torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cache and reduce fragmentation\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "model, train_losses, val_losses = training_loop(model, train_loader, val_loader, optimizer, lr_scheduler, device, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
